\section{Discussion and conclusion} % (fold)
\label{sec:discussion}

MFPCA is a fundamental statistical tool for the analysis of multivariate functional data, which enables us to capture the variability in observations defined by multiple curves. In this paper, we have described the duality between rows and columns of a data matrix within the context of multivariate functional data. We have proposed to use this duality to estimate the eigencomponents of the covariance operator in multivariate functional datasets. By comparing the results of the three methods, we provide the researcher with guidelines for determining the most appropriate method within a range of functional data frameworks. 
%In summary, if the number of sampling points is significantly greater than the number of observations, or if the data are multidimensional (e.g., surfaces), it is preferable to estimate the eigencomponents using the Gram matrix. Conversely, if the data are unidimensional with a large number of observations, the preferred method is the direct decomposition of the covariance operator, regarless of the number of features.
Overall, our simulations showed that the \texttt{(Tensor) PCA}, \texttt{2D/1D B-Splines} and \texttt{Gram} methods give similar results in terms of the estimation of the eigenvalues, the eigenfunctions and reconstruction of the curves. Regarding the computation time, the use of the Gram matrix is faster in most cases. The only situation where the diagonalization of the covariance operator is quicker is when the number of observations is larger than the number of sampling points.
In conclusion, regarding the reconstruction error and computational complexity, if the data are defined on multi-dimensional domains (images) or the number of sampling points is much higher than the number of observations, we advise using the Gram matrix. Another advantage of the \texttt{Gram} method is that it does not require basis expansion, such as the \texttt{2D/1D B-splines} method, or estimate smoothing parameters, such as the \texttt{(Tensor) PCA} method when the data are observed without noise.

In practice, observations of (multivariate) functional data are often subject to noise. In this case, the diagonalization of the covariance operator seems preferable as the curves as to be smoothed beforehand to estimate the Gram matrix and yield similar results, except when the number of sampling is small where the \texttt{Gram} method is preferable. In the case of sparsely sampled functional data, we would advise to use the \texttt{Gram} method, using linear interpolation to estimate the inner-product matrix, as it does not require hyper-parameters estimation and gives slightly better results than the other methods.

Utilizing the Gram matrix enables the estimation of the number of components retained via the percentage of variance explained by the multivariate functional data, whereas the decomposition of the covariance operator necessitates the specification of the percentage of variance accounted for by each individual univariate feature. Specifying the percentage of variance explained for each feature does not guarantee recovery of the nominal percentage of variance explained for the multivariate functional data \citep{golovkineEstimationNumberComponents2023}. Although we have not investigated the extent to which this might be important, the duality relation derived in this work provides a direct solution to the problem. In settings where the univariate variance-explained cutoffs fail to retain the correct percentage of variance explained in multivariate functional data, the Gram matrix approach may be preferred.

The open-source implementation can be accessed at \url{https://github.com/StevenGolovkine/FDApy}, while scripts to reproduce the simulations and data analysis are at \url{https://github.com/FAST-ULxNUIG/geom_mfpca}.


% section discussion (end)