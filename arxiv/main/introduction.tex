%!TeX root=../main.tex
\section{Introduction} % (fold)
\label{sec:introduction}

% ----------------------------------------------------------------------------
% General introduction to FDA and FPCA
% ----------------------------------------------------------------------------
Functional data analysis (FDA) is a statistical methodology for analyzing data that can be represented as functions. These functions could represent measurements taken over time or space, such as temperature readings over a period of time or spatial patterns of disease occurrence. The goal of FDA is to extract meaningful information from these functions and to model their behavior. See, e.g., \cite{ramsayFunctionalDataAnalysis2005,horvathInferenceFunctionalData2012,wangFunctionalDataAnalysis2016,kokoszkaSpecialIssueFunctional2017} for some recent references on functional data analysis.

Functional principal component analysis (FPCA) is an extension of principal component analysis (PCA), which is a commonly used tool for dimension reduction in multivariate data, to functional data. This tool has been introduced by \cite{karhunenUeberLineareMethoden1947} and \cite{loeveFonctionsAleatoiresStationnaires1945} and developed by \cite{dauxoisAsymptoticTheoryPrincipal1982}. Since then, FPCA has become a prevalent tool in FDA due to its ability to convert infinite-dimensional functional data into a finite-dimensional vector of random scores. These scores are a countable sequence of uncorrelated random variables that can be truncated to a finite vector in practical applications. By applying multivariate data analysis tools to these random scores, FPCA can achieve the goal of dimension reduction while assuming mild assumptions about the underlying stochastic process. FPCA is usually used as a preprocessing step to feed regression and classification models. Recently, FPCA has been extended to the analysis of multivariate functional data, which are data that consist of multiple functions that are observed simultaneously. This extension is referred to as multivariate functional data analysis (MFPCA). As for FPCA, a key benefit of MFPCA is that it allows to identify and visualize the main sources of variation in the multivariate functional data. This can be useful in many different applications, such as identifying patterns of movements in sport biomechanics \citep{warmenhovenBivariateFunctionalPrincipal2019}, analyzing changes in brain activity in neuroscience \citep{songSparseMultivariateFunctional2022}, or comparing  countries' competitiveness in economics \citep{krzyskoMultidimensionalEconomicIndicators2022}.

% ----------------------------------------------------------------------------
% General approaches for FPCA and MFPCA
% ----------------------------------------------------------------------------
In MFPCA, we seek to decompose the covariance structure of the multivariate functional data into a set of orthogonal basis functions, named the principal components, which capture the main sources of variation in the data. There are multiple approaches to estimate the principal components of a multivariate functional dataset. \cite{ramsayFunctionalDataAnalysis2005} stack the multivariate curves into one big curve and then perform an usual FPCA by doing an eigendecomposition of the covariance structure. This methodology can only be run for data that are defined on the same unidimensional domain and that exhibit similar variations. \cite{jacquesModelbasedClusteringMultivariate2014a} propose to expand each univariate component into a basis of functions. It results in a set of coefficients for each univariate curve. The eigendecomposition is then run on the matrix of the stacked coefficients. To consider the normalization issue of \cite{ramsayFunctionalDataAnalysis2005}, \cite{jacquesModelbasedClusteringMultivariate2014a} and \cite{chiouMultivariateFunctionalPrincipal2014} propose to normalize the data by the standard deviation of the curves at each sampling points. \cite{happMultivariateFunctionalPrincipal2015} extend the estimation to functional data defined on different dimensional domains. The estimation is based on FPCA for each univariate component and use a weighted combinaison of them to obtain the multivariate eigencomponents. Finally, \cite{berrenderoPrincipalComponentsMultivariate2011} develop a very different method to estimate the eigencomponents as they perform a principal components analysis for each sampling time points.

% ----------------------------------------------------------------------------
% Key motivations of the paper -> Duality between rows and colums of a matrix
% ----------------------------------------------------------------------------
The key motivation of the paper is to use the duality between rows and columns of a data matrix to estimate the eigencomponents of a multivariate functional dataset.

\textcolor{red}{Key motivations of the paper -- Duality between rows and colums of a matrix}
\textcolor{blue}{the duality between rows and columns of a data matrix is a fundamental concept in classical statistics that can be used to understand the relationship between different types of statistical analyses.
In classical statistics, a data matrix typically represents a set of observations of multiple variables, where each row corresponds to an individual observation and each column corresponds to a variable. The duality between rows and columns refers to the fact that many statistical analyses can be conducted either on the rows or the columns of the data matrix, and the resulting analyses will be related to each other in a specific way.
For example, consider a data matrix where the rows represent individual subjects and the columns represent different measurements taken on each subject, such as blood pressure, weight, and age. Conducting a principal component analysis (PCA) on the columns of this data matrix will result in a set of principal components that capture the variability in the different measurements across all subjects. These principal components can be interpreted as patterns of co-variation across the different measurements.
Alternatively, conducting a PCA on the rows of the same data matrix will result in a set of principal components that capture the variability in the measurements across different subjects. These principal components can be interpreted as patterns of co-variation across the different subjects.
The duality between rows and columns means that the principal components obtained from these two different analyses will be related in a specific way. Specifically, the principal components obtained from the column PCA can be used to predict the principal components obtained from the row PCA, and vice versa. This relationship can be expressed mathematically as a matrix transpose operation, where the columns of the original data matrix become the rows of the transposed data matrix, and vice versa.
In the context of multivariate functional principal components analysis (MFPCA), the duality between rows and columns can be used to understand the relationship between the principal components obtained from the analysis of multiple functional variables observed on the same set of subjects or units. This duality is particularly useful when dealing with high-dimensional functional data where it may not be feasible to analyze all the functional variables at once.}

The key motivation of the paper is that in a large number of applications, the number of components of the functional datasets is very large, and estimating the eigencomponents of the covariance operator require the diagonalisation of each univariate component which can be computationaly extensive. Using the duality between rows and colums of the data matrix reduce the number of matrix disgonalisation to only one. Another arguments is that for data defined on multidimensional domains, the estimation of the covariance operator remains unclear (how to compute the covariance of 2-dimensional data?). The use of the inner-product matrix allows the eigencomponents of multidimensional data to be computed.

Duality of usual matrix \cite{escofierTraitementSimultaneVariables1979,saportaSimultaneousAnalysisQualitative1990,pagesAnalyseFactorielleDonnees2004,hardleAppliedMultivariateStatistical2019}


% ----------------------------------------------------------------------------
% Duality for functional data
% ----------------------------------------------------------------------------
\textcolor{red}{Duality in the functional data case}
For functional data, \cite{ramsayWhenDataAre1982a} explains the duality of the space of functions and the space of time points. Used in \cite{benkoCommonFunctionalPrincipal2009} for univariate and unidimensional functional data. \cite{chenQuantifyingInfiniteDimensionalData2017}


% ----------------------------------------------------------------------------
% Rest of the paper
% ----------------------------------------------------------------------------
\textcolor{red}{Remainder of the paper.}
The remainder of the paper is organized as follows. In Section~\ref{sec:model}, we define multivariate functional data with the coordinates possibly having different definition domains. In Section~\ref{sec:geometric_point_of_view_mfpca}, we develop the duality between the observations' space and the functional components space. The relationship between the eigencomponents of the covariance operator of the functional datasets and the eigencomponents of the inner-product matrix between the observations is derived in Section~\ref{sec:functional_principal_components_analysis}. Extensive simulations are given in Section~\ref{sec:simulations}. We also provide guidelines on which method to use with respect to data characteristics. The paper concludes with a discussion and an outlook in Section~\ref{sec:discussion}.

% section introduction (end)