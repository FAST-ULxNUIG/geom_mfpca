%!TEX root=../main.tex
\section{Multivariate functional principal components analysis} % (fold)
\label{sec:functional_principal_components_analysis}

Assuming that the covariance operator $\Gamma$ is a compact positive operator on $\HH$ and using the results in \cite{happMultivariateFunctionalPrincipal2018a}, and the theory of Hilbert-Schmidt operators, e.g., \cite{reedMethodsModernMathematical1980}, there exists a complete orthonormal basis 
$\Phi = \{\phi_k\}_{k \geq 1} \subset \HH$ associated to a set of real numbers $\{\lambda_k\}_{k \geq 1}$ such that $\lambda_1 \geq \lambda_2 \geq \dots \geq 0$ that satisfy
\begin{equation}\label{eq:eigendecomposition}
    \Gamma \phi_k = \lambda_k \phi_k, \quad\text{and}\quad \lambda_k \longrightarrow 0 \quad\text{as}\quad k \longrightarrow \infty.
\end{equation}
The set $\{\lambda_k\}_{k \geq 1}$ contains the eigenvalues of the covariance operator $\Gamma$ and $\Phi$ contains the associated eigenfunctions. Using the multivariate Karhunen-Loève theorem \citep{happMultivariateFunctionalPrincipal2018a}, we obtain the decomposition
\begin{equation}\label{eq:kl_multi}
    X(\pointt) = \mu(\pointt) + \sum_{k = 1}^\infty \mathfrak{c}_k \phi_k(\pointt), \quad \pointt \in \TT{}
\end{equation}
where $\mathfrak{c}_{k} = \inH{X - \mu}{\phi_k}$ are the projections of the centered curve onto the eigenfunctions. We have that $\EE(\mathfrak{c}_{k}) = 0$, $\EE(\mathfrak{c}_{k}^2) = \lambda_k$ and $\EE(\mathfrak{c}_{k}\mathfrak{c}_{k^\prime}) = 0$ for $k \neq k^\prime$. Note that the coefficients $\mathfrak{c}_k$ are scalar random variables while the multivariate functions $\phi_k$ are vectors of functions. Let us call $\Phi$ the multivariate functional principal component analysis basis. In practice, we use a truncated version of the Karhunen-Loève expansion \eqref{eq:kl_multi} as the eigenvalues $\lambda_k$, and hence the contribution of $\mathfrak{c}_k$ to \eqref{eq:kl_multi}, becomes negligible as $k$ goes to infinity. Let
\begin{equation}\label{eq:kl_multi_trunc}
    X_{\lceil K \rceil}(\pointt) = \mu(\pointt) + \sum_{k = 1}^K \mathfrak{c}_k \phi_k(\pointt), \quad \pointt \in \TT{}, \quad K \geq 1,
\end{equation}
be the truncated Karhunen-Loève expansion of the process $X$ and
\begin{equation}\label{eq:kl_multi_trunc_comp}
    X_{\lceil K_p \rceil}^{(p)}(t_p) = \mup{p}(t_p) + \sum_{k = 1}^{K_p} \mathfrak{c}^{(p)}_k \varphi_k^{(p)}(t_p), \quad t_p \in \TT{p}, \quad K_p \geq 1, \quad 1 \leq p \leq P,
\end{equation}
be the truncated Karhunen-Loève expansion of the $p$th feature of the process $X$. For each $p$, the function $\mup{p}$ is the $p$th feature of the multivariate mean function $\mu$ and the set $\{\varphi^{(p)}_k\}_{1 \leq k \leq K_p}$ is a basis of univariate functions in $\sLp{\TT{p}}$, whose elements are not the components of the multivariate functions $\phi_k$. In~\eqref{eq:kl_multi_trunc_comp}, the coefficients $\mathfrak{c}^{(p)}_k$ are the projection of the centered curve $\Xp{p}$ onto the eigenfunctions $\varphi_k^{(p)}$ and are not (directly) related to the coefficients $\mathfrak{c}_k$ in~\eqref{eq:kl_multi_trunc}.


\subsection{Diagonalization of the covariance operator} % (fold)
\label{sub:by_diagonalization_of_the_covariance_operator}

The estimation of the eigencomponents of the covariance $\Gamma$ by its diagonalization is derived in \cite{happMultivariateFunctionalPrincipal2018a} for a general class of multivariate functional data defined on different dimensional domains. They give a direct relationship between the truncated representation \eqref{eq:kl_multi_trunc_comp} of the single elements $X^{(p)}$ and the truncated representation \eqref{eq:kl_multi_trunc} of the multivariate functional data $X$.

We recall here how to estimate the eigencomponents. Following \citet[Prop.~5]{happMultivariateFunctionalPrincipal2018a}, the multivariate components for $X$ are estimated by a weighted combination of the univariate components computed from each $X^{(p)}$. First, we perform a univariate FPCA on each of the features of $X$ separately. For a feature $X^{(p)}$, the eigenfunctions and eigenvectors are computed using a matrix decomposition of the estimated covariance $C_{p, p}$ from \eqref{eq:cov_estimation}. This results in a set of eigenfunctions $\{\varphi_k^{(p)}\}_{1 \leq k \leq K_p}$ associated with a set of eigenvalues $\{\lambda_k^{(p)}\}_{1 \leq k \leq K_p}$ for a given truncation integer $K_p$. Then, the univariate scores for a realization $\Xnp$ of $X^{(p)}$ are given by $\mathbf{c}_{nk}^{(p)} = \inLp{X_n^{(p)}}{\varphi_k^{(p)}}, ~1 \leq k \leq K_p$. These scores might be estimated by numerical integration for example. Considering $K_+ \coloneqq \sum_{p = 1}^P K_p$, we then define the matrix $\mathcal{Z} \in \mathbb{R}^{N \times K_+}$, where on each row we concatenate the scores obtained for the $P$ features of the $n$th observation: 
$(\mathbf{c}_{n1}^{(1)}, \ldots, \mathbf{c}_{nK_1}^{(1)}, \ldots, \mathbf{c}_{n1}^{(P)}, \ldots, \mathbf{c}_{nK_P}^{(P)})$. An estimation of the covariance of the matrix $\mathcal{Z}$ is given by $\mathbf{Z} = (N - 1)^{-1}\mathcal{Z}^\top\mathcal{Z}$. An eigenanalysis of the matrix $\mathbf{Z}$ is carried out to estimate the eigenvectors $\boldsymbol{v}_k$ and eigenvalues $\lambda_k$. Finally, the multivariate eigenfunctions are estimated as a linear combination of the univariate eigenfunctions using
\begin{equation*}
\phi_k^{(p)}(t_p) = \sum_{l = 1}^{K_p}[\boldsymbol{v}_k]_{l}^{(p)}\varphi_{l}^{(p)}(t_p),\quad t_p \in \TT{p},\quad 1 \leq k \leq K_+,\quad 1 \leq p \leq P,
\end{equation*}
where $[\boldsymbol{v}_k]^{(p)}_{l}$ denotes the $l$th entry of the $p$th block of the vector $\boldsymbol{v}_k$. The multivariate scores are estimated as
$$\mathfrak{c}_{nk} = \mathcal{Z}_{{n,\cdot}}\boldsymbol{v}_k, \quad 1 \leq n \leq N, \quad 1 \leq k \leq K_+,$$
where $\mathcal{Z}_{{n,\cdot}}$ is the $n$th row of the matrix $\mathcal{Z}$.
We refer the reader to \cite{happMultivariateFunctionalPrincipal2018a} for the derivation of the eigencomponents of the covariance operator if the curves are expanded in a general basis of functions.

% subsection by_diagonalization_of_the_covariance_operator (end)

\subsection{Diagonalization of the inner product matrix} % (fold)
\label{sub:by_diagonalization_of_the_inner_product_matrix}

We can use the duality relation between row and column spaces of a data matrix to estimate the eigencomponents of the covariance operator. Consider the inner-product matrix $\mathbf{M}$, with entries defined in~\eqref{eq:gram_mat} and assuming that all observations are equally weighted, i.e., for all $n = 1, \dots, N$, $\pi_n = 1/N$.
Let $\{l_k\}_{1 \leq k \leq N}$ such that $l_1 \geq \dots \geq l_N \geq 0$ be the set of eigenvalues and $\{\boldsymbol{u}_k\}_{1 \leq k \leq N}$ be the set of eigenvectors of the matrix $\mathbf{M}$. The relationship between all nonzero eigenvalues of the covariance operator $\Gamma$ and the eigenvalues of $\mathbf{M}$ is given by
\begin{equation}\label{eq:eigenvalues_relation_p}
    \lambda_k = l_k, \quad k = 1, 2, \dots, N,
\end{equation}
while the relationship between the multivariate eigenfunctions of the covariance operator $\Gamma$ and the orthonormal eigenvectors of $M$ is given by
\begin{equation}\label{eq:eigenfunction_relation_p}
    \phi_k(\pointt) = \frac{1}{\sqrt{N l_k}}\sum_{n = 1}^N [\boldsymbol{u}_{k}]_n\left\{X_n(\pointt) - \mu(\pointt)\right\}, \quad \pointt \in \TT{}, \quad k = 1, 2, \dots, N, 
\end{equation}
where $[\boldsymbol{u}_{k}]_n$ is the $n$th entry of the vector $\boldsymbol{u}_k$. The scores are then computed as the inner-product between the multivariate curves and the multivariate eigenfunctions and are given by
\begin{equation}\label{eq:scores_relation_p}
    \mathfrak{c}_{nk} = \sqrt{N l_k}[\boldsymbol{u}_{k}]_n, \quad n = 1, 2, \dots, N, \quad k = 1, 2, \dots, N. 
\end{equation}
The derivations of these equalities are given in Appendix~\ref{sec:derivation_of_the_eigencomponents} in a slighty more general framework where the observation weights $\pi_n$ are not equal. These results can be extended in a natural way if all the curves are expanded in a general basis of functions, see Section \ref{sub:with_a_basis_expansion} in the Supplementary Material.

% subsection by_diagonalization_of_the_inner_product_matrix (end)


\subsection{Computational complexity} % (fold)
\label{sub:computational_complexity}

We describe the time complexity for the computation of the MFPCA algorithm using the covariance operator and the inner product matrix. Considering the observation of $N$ curves with $P$ features, we assume that all observations of the feature $p$ are sampled on a common grid of $M_p$ points. For $a \in \NN$, let $M^a = \sum_{p} M_p^a$. Let $K$ be the number of multivariate eigenfunctions to estimate. For the estimation of the eigencomponents using the covariance operator, we have $K \leq K_+$. While $K$ has the same interpretation for both the eigendecomposition of the covariance operator and the eigendecomposition of the inner product matrix, in the latter case, it is not computed as the summation over the univariate elements, but rather as the number of components needed to achieve a certain amount of variance explained. Here, we also assume that the curves are perfectly observed, and thus no smoothing step is included in the expression of the time complexity. Note that the smoothing step will often have the same impact on complexity between the approaches.

To estimate the time complexity of an algorithm, we count the number of elementary operations performed, considering a fixed execution time for each. Worst-case time complexity is considered. We first give the time complexity for the estimation of the eigencomponents using the covariance operator by explaining the time complexity of each individual step (see \cite{happMultivariateFunctionalPrincipal2018a} and Section~\ref{sub:by_diagonalization_of_the_covariance_operator}). For each feature $p$, the time complexity of the estimation of the covariance matrix is $\bigO(NM_p^2)$, of the eigendecomposition of the matrix is $\bigO(M_p^3)$ and of the univariate score is $\bigO(NM_pK_p)$. Therefore, the total time complexity is the sum over the $p$ univariate time complexities. The covariance matrix $\mathbf{Z}$ of the stacked univariate scores $\mathcal{Z}$ is then computed with a time complexity of $\bigO(NK_+^2)$, because the dimension of the matrix $\mathcal{Z}$ is $N \times K_+$. The eigendecomposition of the matrix $\mathbf{Z}$ has a time complexity of $\bigO(K_+^3)$. The final step is to compute the multivariate eigenfunctions and scores. For the estimation of the $K \leq K_+$ multivariate eigenfunctions, the time complexity is $\bigO(K\sum_{p} M_pK_p)$ and for the estimation of the scores, the time complexity is $\bigO(NK^2)$. Gathering all the results, the final complexity of the estimation of the eigencomponents using the eigendecomposition of the covariance operator is
\begin{equation}\label{eq:time_compl_cov}
    \bigO\left(NM^2 + M^3 + N\sum_{p = 1}^P M_pK_p + NK_+^2 + K_+^3 + K\sum_{p = 1}^P M_pK_p + NK^2\right).
\end{equation}
We now consider the time complexity of the estimation of the eigencomponents using the eigendecomposition of the inner product matrix (see Section~\ref{sub:by_diagonalization_of_the_inner_product_matrix}). The inner product between two curves can be estimated in $\bigO(M^1)$. Since there are $N^2$ terms in the matrix, the time complexity for the computation of the inner product matrix is then $\bigO(N^2M^1)$. The eigendecomposition of this matrix has a time complexity of $\bigO(N^3)$. For the multivariate eigenfunctions, the time complexity is $\bigO(KNP)$ and is $\bigO(KN)$ for the multivariate scores. Gathering all the results, the final complexity of the estimation of eigencomponents using the eigendecomposition of the inner product matrix is
\begin{equation}\label{eq:time_compl_in_prod}
    \bigO\left(N^2M^1 + N^3 + KNP + KN\right).
\end{equation}

The number of components $K$ to estimate is usually small compared to the number of curves $N$ or to the total number of sampling points $M^1$. Both time complexities can then be reduced to $\mathcal{O}(NM^2 + M^3)$ for the diagonalization of the covariance operator and to $\mathcal{O}(N^2M^1 + N^3)$ using the Gram matrix. If the number of observations is large compared to the total number of sampling points, it thus seems preferable to use the covariance operator to estimate the eigencomponents, while if the total number of sampling points is large compare to the number of observations, the use of the Gram matrix seems better. Note that the number of features $P$ does not have much impact on the computational complexity, in the sense that the important part is the total number of sampling points. One component with $1000$ sampling points will have the same computational complexity as $100$ components with $10$ sampling points. These results are confirmed in the simulation (see Section~\ref{sub:simulation_results}).

\begin{remark}
We can use singular values decomposition (SVD) in both cases to make the algorithm faster as it allows to compute only the first $K$ eigenfunctions. In practice, this might be important as the maximum number of non-zero eigenvalues is the minimum between the number of observations and the number of sampling points.
\end{remark}

% subsection computational_complexity (end)

% section functional_principal_components_analysis (end)