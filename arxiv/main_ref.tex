\begin{filecontents}{main.aux}
\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ramsayFunctionalDataAnalysis2005,horvathInferenceFunctionalData2012,wangFunctionalDataAnalysis2016,kokoszkaSpecialIssueFunctional2017}
\citation{karhunenUeberLineareMethoden1947}
\citation{loeveFonctionsAleatoiresStationnaires1945}
\citation{dauxoisAsymptoticTheoryPrincipal1982}
\citation{warmenhovenBivariateFunctionalPrincipal2019}
\citation{songSparseMultivariateFunctional2022}
\citation{krzyskoMultidimensionalEconomicIndicators2022}
\citation{ramsayFunctionalDataAnalysis2005}
\citation{jacquesModelbasedClusteringMultivariate2014a}
\citation{ramsayFunctionalDataAnalysis2005}
\citation{jacquesModelbasedClusteringMultivariate2014a}
\citation{chiouMultivariateFunctionalPrincipal2014}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{berrenderoPrincipalComponentsMultivariate2011}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{escofierTraitementSimultaneVariables1979,saportaSimultaneousAnalysisQualitative1990}
\citation{pagesMultipleFactorAnalysis2014,hardleAppliedMultivariateStatistical2019}
\citation{ramsayWhenDataAre1982a}
\citation{ramsayFunctionalDataAnalysis2005}
\citation{kneipInferenceDensityFamilies2001}
\citation{benkoCommonFunctionalPrincipal2009}
\citation{chenQuantifyingInfiniteDimensionalData2017}
\citation{happMultivariateFunctionalPrincipal2018a}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model}{2}{section.2}\protected@file@percent }
\newlabel{sec:model}{{2}{2}{Model}{section.2}{}}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{berrenderoPrincipalComponentsMultivariate2011}
\citation{berrenderoPrincipalComponentsMultivariate2011}
\citation{chiouMultivariateFunctionalPrincipal2014}
\citation{happMultivariateFunctionalPrincipal2018a}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Functional data matrix, adapted from \cite  {berrenderoPrincipalComponentsMultivariate2011}.}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:data_matrix}{{1}{3}{Functional data matrix, adapted from \cite {berrenderoPrincipalComponentsMultivariate2011}}{figure.caption.1}{}}
\newlabel{eq:innerprodH}{{2}{3}{Model}{section.2}{}}
\newlabel{eq:covariance_function}{{2}{3}{Model}{section.2}{}}
\newlabel{eq:covariance_function_components}{{2}{3}{Model}{section.2}{}}
\newlabel{eq:covariance_operator_components}{{2}{3}{Model}{section.2}{}}
\newlabel{eq:gram_mat}{{1}{3}{Model}{equation.2.1}{}}
\newlabel{eq:innerprodH_weight}{{2}{3}{Model}{figure.caption.1}{}}
\newlabel{eq:covariance_operator_components_weight}{{2}{3}{Model}{figure.caption.1}{}}
\citation{eilersTwentyYearsPsplines2015}
\citation{fanLocalPolynomialModelling1996}
\citation{yaoFunctionalDataAnalysis2005,zhangSparseDenseFunctional2016}
\citation{benkoCommonFunctionalPrincipal2009}
\citation{grithFunctionalPrincipalComponent2018}
\newlabel{eq:gram_mat_weights}{{2}{4}{Model}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Inference}{4}{subsection.2.1}\protected@file@percent }
\newlabel{sub:inference}{{2.1}{4}{Inference}{subsection.2.1}{}}
\newlabel{eq:perfect_estimator}{{2.1}{4}{Inference}{subsection.2.1}{}}
\newlabel{eq:perfect_gram_estimator}{{2.1}{4}{Inference}{subsection.2.1}{}}
\newlabel{eq:model_error}{{2}{4}{Inference}{equation.2.2}{}}
\newlabel{eq:cov_estimation}{{3}{4}{Inference}{equation.2.3}{}}
\citation{hallVarianceEstimationNonparametric1990}
\citation{hallAsymptoticallyOptimalDifferenceBased1990}
\citation{holmesMultivariateDataAnalysis2008}
\citation{delacruzDualityDiagramData2011}
\citation{gonzalezRepresentingFunctionalData2010}
\citation{wongNonparametricOperatorregularizedCovariance2019a}
\@writefile{toc}{\contentsline {section}{\numberline {3}On the geometry of multivariate functional data}{5}{section.3}\protected@file@percent }
\newlabel{sec:geometric_point_of_view_mfpca}{{3}{5}{On the geometry of multivariate functional data}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Duality diagram}{5}{subsection.3.1}\protected@file@percent }
\newlabel{sub:duality_diagram}{{3.1}{5}{Duality diagram}{subsection.3.1}{}}
\newlabel{lm:adjoint}{{1}{5}{}{lm.1}{}}
\newlabel{rem:rhks}{{3}{5}{}{rem.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Duality diagram between the spaces $\mathcal  {H}$ and $\mathbb  {R}^N$. The operator $L_X$ and its adjoint $L^\star _X$ are linear operators. The covariance operator $\Gamma $ and the matrix $\mathbf  {M}$ define geometries in $\mathcal  {H}$ and $\mathbb  {R}^N$ respectively. The space $\mathcal  {H}^\star $ (resp. $\mathbb  {R}^{N \star }$) is the dual space of $\mathcal  {H}$ (resp. $\mathbb  {R}^N$).}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:duality_diagram}{{2}{6}{Duality diagram between the spaces $\mathcal {H}$ and $\mathbb {R}^N$. The operator $L_X$ and its adjoint $L^\star _X$ are linear operators. The covariance operator $\Gamma $ and the matrix $\mathbf {M}$ define geometries in $\mathcal {H}$ and $\mathbb {R}^N$ respectively. The space $\HH ^\star $ (resp. $\RR ^{N \star }$) is the dual space of $\HH $ (resp. $\RR ^N$)}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Left: Cloud of observations. Right: Projection of the points onto the elements of $\mathcal  {H}$. The observation $f$ (resp. $g$) is identified by the point $\mathrm  {M}_f$ (resp. $\mathrm  {M}_g$) in the cloud $\mathcal  {C}_{\!P}$. The point $\mathrm  {G}_{\!\mu }$ is the center of gravity of $\mathcal  {C}_{\!P}$ and the point $\mathrm  {O}_{\!\mathcal  {H}}$ is the origin of the space $\mathcal  {H}$.}}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:cloud_obs}{{3}{6}{Left: Cloud of observations. Right: Projection of the points onto the elements of $\HH $. The observation $f$ (resp. $g$) is identified by the point $\pobs {M}_f$ (resp. $\pobs {M}_g$) in the cloud $\CP $. The point $\Gmu $ is the center of gravity of $\CP $ and the point $\OH $ is the origin of the space $\HH $}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Cloud of individuals}{6}{subsection.3.2}\protected@file@percent }
\newlabel{sub:cloud_of_individuals}{{3.2}{6}{Cloud of individuals}{subsection.3.2}{}}
\newlabel{eq:distance_obs}{{3.2}{6}{Cloud of individuals}{figure.caption.3}{}}
\newlabel{eq:distance_center}{{3.2}{6}{Cloud of individuals}{figure.caption.3}{}}
\citation{berrenderoMahalanobisDistanceFunctional2020,martinoKmeansProcedureBased2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Left: Cloud of features. Right: Projection of the points on the elements of $\mathbb  {R}^N$. The observation $f$ (resp. $g$) is identified by the point $\mathsf  {M}_f$ (resp. $\mathsf  {M}_g$) in the cloud $\mathcal  {C}_{\!N}$. The point $\mathsf  {G}_{\!\mu }$ is the center of gravity of $\mathcal  {C}_{\!N}$ and the point $\mathsf  {O}_{\!\mathbb  {R}}$ is the origin of the space $\mathbb  {R}^N$.}}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:cloud_features}{{4}{7}{Left: Cloud of features. Right: Projection of the points on the elements of $\RR ^N$. The observation $f$ (resp. $g$) is identified by the point $\pfea {M}_f$ (resp. $\pfea {M}_g$) in the cloud $\CN $. The point $\Gfea $ is the center of gravity of $\CN $ and the point $\OG $ is the origin of the space $\RR ^N$}{figure.caption.4}{}}
\newlabel{eq:inertia}{{4}{7}{Cloud of individuals}{equation.3.4}{}}
\newlabel{eq:inertia_CP}{{5}{7}{Cloud of individuals}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Cloud of features}{7}{subsection.3.3}\protected@file@percent }
\newlabel{sub:cloud_of_features}{{3.3}{7}{Cloud of features}{subsection.3.3}{}}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{protheroNewPerspectivesCentering2023}
\citation{protheroNewPerspectivesCentering2023}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{chiouMultivariateFunctionalPrincipal2014}
\citation{jacquesModelbasedClusteringMultivariate2014a}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{reedMethodsModernMathematical1980}
\newlabel{eq:inertia_CN}{{6}{8}{Cloud of features}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}On centering and reducing}{8}{subsection.3.4}\protected@file@percent }
\newlabel{sub:on_centering_and_reducing}{{3.4}{8}{On centering and reducing}{subsection.3.4}{}}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{happMultivariateFunctionalPrincipal2018a}
\@writefile{toc}{\contentsline {section}{\numberline {4}Multivariate functional principal components analysis}{9}{section.4}\protected@file@percent }
\newlabel{sec:functional_principal_components_analysis}{{4}{9}{Multivariate functional principal components analysis}{section.4}{}}
\newlabel{eq:eigendecomposition}{{7}{9}{Multivariate functional principal components analysis}{equation.4.7}{}}
\newlabel{eq:kl_multi}{{8}{9}{Multivariate functional principal components analysis}{equation.4.8}{}}
\MT@newlabel{eq:kl_multi}
\MT@newlabel{eq:kl_multi}
\newlabel{eq:kl_multi_trunc}{{9}{9}{Multivariate functional principal components analysis}{equation.4.9}{}}
\newlabel{eq:kl_multi_trunc_comp}{{10}{9}{Multivariate functional principal components analysis}{equation.4.10}{}}
\MT@newlabel{eq:kl_multi_trunc_comp}
\MT@newlabel{eq:kl_multi_trunc}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Diagonalization of the covariance operator}{9}{subsection.4.1}\protected@file@percent }
\newlabel{sub:by_diagonalization_of_the_covariance_operator}{{4.1}{9}{Diagonalization of the covariance operator}{subsection.4.1}{}}
\MT@newlabel{eq:kl_multi_trunc_comp}
\MT@newlabel{eq:kl_multi_trunc}
\MT@newlabel{eq:cov_estimation}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{happMultivariateFunctionalPrincipal2018a}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Diagonalization of the inner product matrix}{10}{subsection.4.2}\protected@file@percent }
\newlabel{sub:by_diagonalization_of_the_inner_product_matrix}{{4.2}{10}{Diagonalization of the inner product matrix}{subsection.4.2}{}}
\MT@newlabel{eq:gram_mat}
\newlabel{eq:eigenvalues_relation_p}{{11}{10}{Diagonalization of the inner product matrix}{equation.4.11}{}}
\newlabel{eq:eigenfunction_relation_p}{{12}{10}{Diagonalization of the inner product matrix}{equation.4.12}{}}
\newlabel{eq:scores_relation_p}{{13}{10}{Diagonalization of the inner product matrix}{equation.4.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Computational complexity}{10}{subsection.4.3}\protected@file@percent }
\newlabel{sub:computational_complexity}{{4.3}{10}{Computational complexity}{subsection.4.3}{}}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{allenMultiwayFunctionalPrincipal2013a}
\citation{eilersFlexibleSmoothingBsplines1996}
\newlabel{eq:time_compl_cov}{{4.3}{11}{Computational complexity}{subsection.4.3}{}}
\newlabel{eq:time_compl_in_prod}{{4.3}{11}{Computational complexity}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical analysis}{11}{section.5}\protected@file@percent }
\newlabel{sec:empirical_analysis}{{5}{11}{Empirical analysis}{section.5}{}}
\newlabel{eq:ise_eigenfunctions}{{14}{11}{Empirical analysis}{equation.5.14}{}}
\newlabel{eq:mse_eigenvalues}{{15}{11}{Empirical analysis}{equation.5.15}{}}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{happMultivariateFunctionalPrincipal2018a}
\citation{yaoFunctionalDataAnalysis2005}
\citation{eilersFlexibleSmoothingBsplines1996}
\citation{benkoCommonFunctionalPrincipal2009}
\newlabel{eq:mise_reconstructed_data}{{16}{12}{Empirical analysis}{equation.5.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Simulation experiments}{12}{subsection.5.1}\protected@file@percent }
\newlabel{sub:simulation_experiments}{{5.1}{12}{Simulation experiments}{subsection.5.1}{}}
\MT@newlabel{eq:kl_multi_trunc}
\MT@newlabel{eq:model_error}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Simulation results}{12}{subsection.5.2}\protected@file@percent }
\newlabel{sub:simulation_results}{{5.2}{12}{Simulation results}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Ratio of computation time in the dense case between the \texttt  {(Tensor) PCA} and \texttt  {2D/1D B-Splines} methods and the \texttt  {Gram} method. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves).}}{13}{figure.caption.5}\protected@file@percent }
\newlabel{fig:computation_time_mfd_1d}{{5}{13}{Ratio of computation time in the dense case between the \texttt {(Tensor) PCA} and \texttt {2D/1D B-Splines} methods and the \texttt {Gram} method. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves)}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces RSE for the estimated eigenvalues for each method in the dense case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves).}}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:logAE_mfd_1d}{{6}{13}{RSE for the estimated eigenvalues for each method in the dense case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves)}{figure.caption.6}{}}
\MT@newlabel{eq:mse_eigenvalues}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ISE for the estimated eigenfunctions for each method in the dense case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves).}}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:ise_mfd_1d}{{7}{14}{ISE for the estimated eigenfunctions for each method in the dense case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves)}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MRSE for the reconstructed curves for each method in the dense case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves).}}{14}{figure.caption.8}\protected@file@percent }
\newlabel{fig:mise_mfd_1d}{{8}{14}{MRSE for the reconstructed curves for each method in the dense case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves)}{figure.caption.8}{}}
\MT@newlabel{eq:ise_eigenfunctions}
\MT@newlabel{eq:mise_reconstructed_data}
\citation{silvermanDensityEstimationStatistics1986}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Made/Missed shots chart (left) and the estimated densities (right) for Stephen Curry.}}{15}{figure.caption.9}\protected@file@percent }
\newlabel{fig:shoots_make_miss}{{9}{15}{Made/Missed shots chart (left) and the estimated densities (right) for Stephen Curry}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Application}{15}{section.6}\protected@file@percent }
\newlabel{sec:application}{{6}{15}{Application}{section.6}{}}
\citation{golovkineEstimationNumberComponents2023}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The estimated mean surfaces (first column) and the estimated eigenfunctions (second to fifth columns) for the shots dataset using the \texttt  {Gram} method.}}{16}{figure.caption.10}\protected@file@percent }
\newlabel{fig:eigenfunctions_gram}{{10}{16}{The estimated mean surfaces (first column) and the estimated eigenfunctions (second to fifth columns) for the shots dataset using the \texttt {Gram} method}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and conclusion}{16}{section.7}\protected@file@percent }
\newlabel{sec:discussion}{{7}{16}{Discussion and conclusion}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Derivation of the equalities}{17}{appendix.A}\protected@file@percent }
\newlabel{sec:derivation_of_the_inertia_of_the_clouds}{{A}{17}{Derivation of the equalities}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Proof of Lemma \ref {lm:adjoint}}{17}{subsection.A.1}\protected@file@percent }
\newlabel{sub:proof_of_lemma_lm:adjoint}{{A.1}{17}{Proof of Lemma \ref {lm:adjoint}}{subsection.A.1}{}}
\newlabel{eq:adjoint_op}{{17}{17}{Proof of Lemma \ref {lm:adjoint}}{equation.A.17}{}}
\MT@newlabel{eq:adjoint_op}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Derivation of the inertia of the clouds}{17}{subsection.A.2}\protected@file@percent }
\newlabel{sub:derivation_of_the_inertia_of_the_clouds}{{A.2}{17}{Derivation of the inertia of the clouds}{subsection.A.2}{}}
\newlabel{eq:var_appendix}{{A.2}{17}{Derivation of the inertia of the clouds}{subsection.A.2}{}}
\MT@newlabel{eq:inertia}
\MT@newlabel{eq:inertia}
\MT@newlabel{eq:inertia_CP}
\MT@newlabel{eq:inertia_CP}
\MT@newlabel{eq:inertia_CN}
\MT@newlabel{eq:inertia_CN}
\@writefile{toc}{\contentsline {section}{\numberline {B}Derivation of the eigencomponents}{19}{appendix.B}\protected@file@percent }
\newlabel{sec:derivation_of_the_eigencomponents}{{B}{19}{Derivation of the eigencomponents}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}General framework}{19}{subsection.B.1}\protected@file@percent }
\newlabel{sub:general_framework}{{B.1}{19}{General framework}{subsection.B.1}{}}
\MT@newlabel{eq:eigenvalues_relation_p}
\MT@newlabel{eq:eigenfunction_relation_p}
\MT@newlabel{eq:scores_relation_p}
\newlabel{eq:eigen_inner_prod_p}{{18}{19}{General framework}{equation.B.18}{}}
\MT@newlabel{eq:eigen_inner_prod_p}
\newlabel{eq:eigen_inner_prod_left}{{19}{19}{General framework}{equation.B.19}{}}
\MT@newlabel{eq:eigen_inner_prod_left}
\newlabel{eq:inner_prod_p}{{20}{19}{General framework}{equation.B.20}{}}
\MT@newlabel{eq:inner_prod_p}
\newlabel{eq:inner_prod_p_left}{{21}{20}{General framework}{equation.B.21}{}}
\MT@newlabel{eq:inner_prod_p}
\newlabel{eq:inner_prod_p_right}{{22}{20}{General framework}{equation.B.22}{}}
\MT@newlabel{eq:inner_prod_p_left}
\MT@newlabel{eq:inner_prod_p_right}
\MT@newlabel{eq:eigendecomposition}
\newlabel{eq:eigen_estimation}{{23}{20}{General framework}{equation.B.23}{}}
\MT@newlabel{eq:eigen_estimation}
\MT@newlabel{eq:eigenvalues_relation_p}
\MT@newlabel{eq:eigenfunction_relation_p}
\MT@newlabel{eq:scores_relation_p}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces RSE for the estimated eigenvalues for each method in the noisy case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves).}}{21}{figure.caption.11}\protected@file@percent }
\newlabel{fig:logAE_mfd_1d_noise}{{11}{21}{RSE for the estimated eigenvalues for each method in the noisy case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves)}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces RSE for the estimated eigenvalues for each method in the sparse case. We set $N = 250$, $M^{(1)} = 101 \times 51$ and $M^{(2)} = 201$. For the case of medium sparsity, we remove $50\%-70\%$ of the sampling points and for the case of high sparsity, we remove $90\%-95\%$ of the sampling points.}}{21}{figure.caption.12}\protected@file@percent }
\newlabel{fig:logAE_mfd_1d_sparse}{{12}{21}{RSE for the estimated eigenvalues for each method in the sparse case. We set $N = 250$, $M^{(1)} = 101 \times 51$ and $M^{(2)} = 201$. For the case of medium sparsity, we remove $50\%-70\%$ of the sampling points and for the case of high sparsity, we remove $90\%-95\%$ of the sampling points}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}More results}{21}{appendix.C}\protected@file@percent }
\newlabel{sec:more_results}{{C}{21}{More results}{appendix.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Simulation}{21}{subsection.C.1}\protected@file@percent }
\newlabel{sub:simulation}{{C.1}{21}{Simulation}{subsection.C.1}{}}
\MT@newlabel{eq:model_error}
\bibstyle{apalike}
\bibdata{./biblio.bib}
\bibcite{allenMultiwayFunctionalPrincipal2013a}{{1}{2013}{{Allen}}{{}}}
\bibcite{benkoCommonFunctionalPrincipal2009}{{2}{2009}{{Benko et~al.}}{{}}}
\bibcite{berrenderoPrincipalComponentsMultivariate2011}{{3}{2011}{{Berrendero et~al.}}{{}}}
\bibcite{berrenderoMahalanobisDistanceFunctional2020}{{4}{2020}{{Berrendero et~al.}}{{}}}
\bibcite{chenQuantifyingInfiniteDimensionalData2017}{{5}{2017}{{Chen et~al.}}{{}}}
\bibcite{chiouMultivariateFunctionalPrincipal2014}{{6}{2014}{{Chiou et~al.}}{{}}}
\bibcite{dauxoisAsymptoticTheoryPrincipal1982}{{7}{1982}{{Dauxois et~al.}}{{}}}
\bibcite{delacruzDualityDiagramData2011}{{8}{2011}{{{De la Cruz} and Holmes}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces ISE for the estimated eigenfunctions for each method in the noisy case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves).}}{22}{figure.caption.13}\protected@file@percent }
\newlabel{fig:ise_mfd_1d_noise}{{13}{22}{ISE for the estimated eigenfunctions for each method in the noisy case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves)}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Application}{22}{subsection.C.2}\protected@file@percent }
\newlabel{sub:application}{{C.2}{22}{Application}{subsection.C.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces ISE for the estimated eigenfunctions for each method in the sparse case. We set $N = 250$, $M^{(1)} = 101 \times 51$ and $M^{(2)} = 201$. For the case of medium sparsity, we remove $50\%-70\%$ of the sampling points and for the case of high sparsity, we remove $90\%-95\%$ of the sampling points.}}{23}{figure.caption.14}\protected@file@percent }
\newlabel{fig:ise_mfd_1d_sparse}{{14}{23}{ISE for the estimated eigenfunctions for each method in the sparse case. We set $N = 250$, $M^{(1)} = 101 \times 51$ and $M^{(2)} = 201$. For the case of medium sparsity, we remove $50\%-70\%$ of the sampling points and for the case of high sparsity, we remove $90\%-95\%$ of the sampling points}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces MRSE for the reconstructed curves for each method in the noisy case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves).}}{23}{figure.caption.15}\protected@file@percent }
\newlabel{fig:mise_mfd_1d_noise}{{15}{23}{MRSE for the reconstructed curves for each method in the noisy case. $N$ is the number of observations, $M$ is the number of sampling points per curve (the first two numbers are for the images and the last one is for the curves)}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces MRSE for the reconstructed curves for each method in the sparse case. We set $N = 250$, $M^{(1)} = 101 \times 51$ and $M^{(2)} = 201$. For the case of medium sparsity, we remove $50\%-70\%$ of the sampling points and for the case of high sparsity, we remove $90\%-95\%$ of the sampling points.}}{23}{figure.caption.16}\protected@file@percent }
\newlabel{fig:mise_mfd_1d_sparse}{{16}{23}{MRSE for the reconstructed curves for each method in the sparse case. We set $N = 250$, $M^{(1)} = 101 \times 51$ and $M^{(2)} = 201$. For the case of medium sparsity, we remove $50\%-70\%$ of the sampling points and for the case of high sparsity, we remove $90\%-95\%$ of the sampling points}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The estimated mean surfaces (first column) and the estimated eigenfunctions (second to fifth columns) for the shots dataset using the \texttt  {(Tensor) PCA} method.}}{24}{figure.caption.17}\protected@file@percent }
\newlabel{fig:eigenfunctions_fcptpa}{{17}{24}{The estimated mean surfaces (first column) and the estimated eigenfunctions (second to fifth columns) for the shots dataset using the \texttt {(Tensor) PCA} method}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces The estimated mean surfaces (first column) and the estimated eigenfunctions (second to fifth columns) for the shots dataset using the \texttt  {2D/1D B-splines} method.}}{24}{figure.caption.18}\protected@file@percent }
\newlabel{fig:eigenfunctions_psplines}{{18}{24}{The estimated mean surfaces (first column) and the estimated eigenfunctions (second to fifth columns) for the shots dataset using the \texttt {2D/1D B-splines} method}{figure.caption.18}{}}
\bibcite{eilersFlexibleSmoothingBsplines1996}{{9}{1996}{{Eilers and Marx}}{{}}}
\bibcite{eilersTwentyYearsPsplines2015}{{10}{2015}{{Eilers et~al.}}{{}}}
\bibcite{escofierTraitementSimultaneVariables1979}{{11}{1979}{{Escofier}}{{}}}
\bibcite{fanLocalPolynomialModelling1996}{{12}{1996}{{Fan and Gijbels}}{{}}}
\bibcite{golovkineEstimationNumberComponents2023}{{13}{2023}{{Golovkine et~al.}}{{}}}
\bibcite{gonzalezRepresentingFunctionalData2010}{{14}{2010}{{Gonz{\'a}lez and Mu{\~n}oz}}{{}}}
\bibcite{grithFunctionalPrincipalComponent2018}{{15}{2018}{{Grith et~al.}}{{}}}
\bibcite{hallAsymptoticallyOptimalDifferenceBased1990}{{16}{1990}{{Hall et~al.}}{{}}}
\bibcite{hallVarianceEstimationNonparametric1990}{{17}{1990}{{Hall and Marron}}{{}}}
\bibcite{happMultivariateFunctionalPrincipal2018a}{{18}{2018}{{Happ and Greven}}{{}}}
\bibcite{hardleAppliedMultivariateStatistical2019}{{19}{2019}{{H{\"a}rdle and Simar}}{{}}}
\bibcite{holmesMultivariateDataAnalysis2008}{{20}{2008}{{Holmes}}{{}}}
\bibcite{horvathInferenceFunctionalData2012}{{21}{2012}{{Horv{\`a}th and Kokoszka}}{{}}}
\bibcite{jacquesModelbasedClusteringMultivariate2014a}{{22}{2014}{{Jacques and Preda}}{{}}}
\bibcite{karhunenUeberLineareMethoden1947}{{23}{1947}{{Karhunen}}{{}}}
\bibcite{kneipInferenceDensityFamilies2001}{{24}{2001}{{Kneip and Utikal}}{{}}}
\bibcite{kokoszkaSpecialIssueFunctional2017}{{25}{2017}{{Kokoszka et~al.}}{{}}}
\bibcite{krzyskoMultidimensionalEconomicIndicators2022}{{26}{2022}{{Krzy{\'s}ko et~al.}}{{}}}
\bibcite{loeveFonctionsAleatoiresStationnaires1945}{{27}{1945}{{Lo{\`e}ve}}{{}}}
\bibcite{martinoKmeansProcedureBased2019}{{28}{2019}{{Martino et~al.}}{{}}}
\bibcite{pagesMultipleFactorAnalysis2014}{{29}{2014}{{Pag{\`e}s}}{{}}}
\bibcite{protheroNewPerspectivesCentering2023}{{30}{2023}{{Prothero et~al.}}{{}}}
\bibcite{ramsayFunctionalDataAnalysis2005}{{31}{2005}{{Ramsay and Silverman}}{{}}}
\bibcite{ramsayWhenDataAre1982a}{{32}{1982}{{Ramsay}}{{}}}
\bibcite{reedMethodsModernMathematical1980}{{33}{1980}{{Reed and Simon}}{{}}}
\bibcite{saportaSimultaneousAnalysisQualitative1990}{{34}{1990}{{Saporta}}{{}}}
\bibcite{silvermanDensityEstimationStatistics1986}{{35}{1986}{{Silverman}}{{}}}
\bibcite{songSparseMultivariateFunctional2022}{{36}{2022}{{Song and Kim}}{{}}}
\bibcite{wangFunctionalDataAnalysis2016}{{37}{2016}{{Wang et~al.}}{{}}}
\bibcite{warmenhovenBivariateFunctionalPrincipal2019}{{38}{2019}{{Warmenhoven et~al.}}{{}}}
\bibcite{wongNonparametricOperatorregularizedCovariance2019a}{{39}{2019}{{Wong and Zhang}}{{}}}
\bibcite{yaoFunctionalDataAnalysis2005}{{40}{2005}{{Yao et~al.}}{{}}}
\bibcite{zhangSparseDenseFunctional2016}{{41}{2016}{{Zhang and Wang}}{{}}}
\gdef \@abspage@last{26}
\end{filecontents}